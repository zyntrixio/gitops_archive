---
groups:

# Set London time
  - name: daylight_saving
    rules:
      - record: is_european_summer_time
        expr: |
          (vector(1) and (month() > 3 and month() < 10))
          or
          (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          vector(0)
      - record: europe_london_hour
        expr: hour() + is_european_summer_time

  - name: Infrastructure
    rules:
      - alert: Unavailable Replicas
        expr: sum by(deployment, namespace) (kube_deployment_status_replicas_unavailable)
          > 0
        for: 5m
        labels:
          severity: p2
          cluster: prod
          component: Kubernetes
          resolve: true
        annotations:
          title: Prod - {{ $labels.deployment}}
          description: 'Deployment: {{ $labels.deployment}} in Namespace: {{$labels.namespace}}
            has unavailable replicas.'

      - alert: Container Restarts
        expr: increase(kube_pod_container_status_restarts_total{namespace!="kube-system"}[1h]) > 1.01
        for: 1s
        labels:
          severity: p2
          cluster: prod
          resolve: true
          component: Kubernetes
        annotations:
          title: Prod - Container restarts
          description: 'Container: {{$labels.container}} in Deploy: {{$labels.pod}}
            in Namespace: {{$labels.namespace}} is restarting alot'

      - alert: Kube pod waiting status
        expr: sum by (reason, pod)(kube_pod_container_status_waiting_reason) > 0
        for: 5m
        labels:
          severity: p2
          cluster: prod
          resolve: true
          component: Kubernetes
        annotations:
          title: Prod - Pod waiting status
          description: 'Pod: {{$labels.pod}} is in this State: {{$labels.reason}}
            in this Namespace: {{$labels.namespace}}, in this cluster: {{$labels.kubernetes_cluster}}'

      # - alert: Rabbitmq Queue Length Olympus
      #   expr: sum by(queue)(rabbitmq_queue_messages_ready{queue!~"hubble-activities|polaris-transaction-history|hubble-activites-dlx-queue"}) > 100
      #   for: 5m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #     component: Rabbitmq
      #   annotations:
      #     title: Olympus Rabbitmq queue length over 100 ready tasks
      #     description: 'Olympus - {{$labels.queue}} queue has been at over 100 tasks for 5 mins'

      # - alert: Rabbitmq Queue Length BPL
      #   expr: sum by(queue)(rabbitmq_queue_messages_ready{queue=~"hubble-activities|polaris-transaction-history"}) > 5
      #   for: 5m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #     component: Rabbitmq
      #   annotations:
      #     title: BPL Rabbitmq queue length over 100 ready tasks
      #     description: 'BPL - {{$labels.queue}} queue has been at over 5 tasks for 5 mins'

      # - alert: Rabbitmq Queue Length Deadletter
      #   expr: sum by(queue)(rabbitmq_queue_messages_ready{queue=~"hubble-activites-dlx-queue"}) > 0
      #   for: 1s
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #     component: Rabbitmq
      #   annotations:
      #     title: Deadletter Rabbitmq queue length over 0 tasks
      #     description: 'Deadletter {{$labels.queue}} queue is over 0 tasks'

      # - alert: Copybot Hermes - No Jobs Processed
      #   expr: increase(events_processed_total{job="copybot-hermes"}[10m]) < 1
      #   for: 1s
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #     component: Copybot Hermes
      #   annotations:
      #     title: Copybot Hermes hasn't processed any tasks for 10 minutes
      #     description: Copybot Hermes hasn't processed any tasks for 10 minutes

      # - alert: Copybot Harmonia - No Jobs Processed
      #   expr: increase(events_processed_total{job="copybot-Harmonia"}[10m]) < 1
      #   for: 1s
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #     component: Copybot Harmonia
      #   annotations:
      #     title: Copybot Harmonia hasn't processed any tasks for 10 minutes
      #     description: Copybot Harmonia hasn't processed any tasks for 10 minutes

      # - alert: core sftp up state
      #   expr: up{job="azure_node",name=~"sftp\\d"} < 1
      #   for: 3m
      #   labels:
      #     severity: S1
      #   annotations:
      #     resource: '{{ $labels.name }}'
      #     summary: Check sftp virtual machines - bad up state for over 3 minutes

  - name: Service Team
    rules:
      - alert: Exported transactions
        expr: sum(increase(transactions_total{slug!="iceland-bonus-card"}[2h])) by
          (slug) < 1 and europe_london_hour != 12 < 23
        for: 1m
        labels:
          severity: p2
          cluster: prod
          component: harmonia
          resolve: false
        annotations:
          resource: '{{ $labels.slug }}'
          sumamry: There have been no exports between 12:00 and 23:00

      # - alert: Payment cards processing slowly
      #   expr: (sum(payment_card_processing_seconds_histogram_sum{}) by (provider)
      #     / sum(payment_card_processing_seconds_histogram_count{}) by (provider))
      #     > 5
      #   for: 5m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     component: hermes
      #     resolve: false
      #   annotations:
      #     summany: Payment cards for {{ $labels.provider }} are taking longer than
      #       four seconds to process.
      # - alert: Payment cards stuck increase
      #   expr: sum by (provider) (hermes_current_payment_card_pending_overdue_total{})
      #     > sum by (provider) (hermes_current_payment_card_pending_overdue_total{}
      #     offset 24h +15)
      #   for: 5m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     component: asteria
      #     resolve: false
      #   annotations:
      #     resource: '{{$labels.provider}}'
      #     summary: Payment cards have been stuck in a pending state for more than
      #       24 hours

      - alert: Vop status stuck in transitory state
        expr: sum by (status)(hermes_current_vop_activation_status_total{status=~"Deactivating|Activating"})
          > sum by (status)(hermes_current_vop_activation_status_total{status=~"Deactivating|Activating"}
          offset 15m)
        for: 15m
        labels:
          severity: p2
          cluster: prod
          component: asteria
          resolve: false
        annotations:
          resource: '{{$labels.status}}'
          summary: VOP account status in {{$labels.status}} state for over 15 minutes
            - check

      # - alert: Failed exports - receipt number not found
      #   expr: sum(failed_requests_total{response_result="receipt no not found"}) >
      #     500
      #   for: 1m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     component: harmonia
      #     resolve: false
      #   annotations:
      #     resource: '{{ $labels.view }} - {{ $labels.status }} - {{ $labels.slug }}
      #       - {{ $labels.response_result }}'
      #     summary: There have been failed exports from harmonia returning receipt
      #       not found.

      - alert: Failed exports excluding receipt not found
        expr: sum(failed_requests_total{response_result!="receipt no not found"})
          > 100
        for: 1m
        labels:
          severity: p2
          cluster: prod
          component: harmonia
          resolve: false
        annotations:
          resource: '{{ $labels.view }} - {{ $labels.status }} - {{ $labels.slug }}
            - {{ $labels.response_result }}'
          summary: There have been failed exports from harmonia.

      - alert: Failed exports - "an error has occured"
        expr: sum(increase(failed_requests_total{response_result="an error has occurred."}
          [10m])) > 0
        for: 1m
        labels:
          severity: p2
          cluster: prod
          component: harmonia
          resolve: false
        annotations:
          resource: '{{ $labels.view }} - {{ $labels.status }} - {{ $labels.slug }}
            - {{ $labels.response_result }}'
          summary: There have been failed exports for wasabi.

      - alert: midas - failed http requests to merchants
        expr: sum(increase(request_fail_total{}[1h])) by (error, slug) > 6
        for: 90m
        labels:
          severity: p2
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.slug }} - {{ $labels.error }}'
          summary: There has been an increase in failed HTTP requests from Midas to
            {{$labels.slug}}

      - alert: midas - failed login is increasing
        expr: sum(increase(log_in_fail_total{}[10m])) by (slug) > 5
        for: 15m
        labels:
          severity: p2
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.slug }}'
          summary: There has been an increase of 5 in failed logins in 10 mins

      # - alert: Files not received - wasabi
      #   expr: (increase(blobstorage_files_total{name="wasabihack"}[46m]) or on() vector(0)) < 1 and ON() hour() > 13 < 23
      #   for: 3m
      #   labels:
      #     severity: p2
      #     cluster: prod
      #     resolve: false
      #   annotations:
      #     resource: '{{ $labels.name }}'
      #     summary: Three missing files over 45 minute period - check latest Wasabi import (Transaction log lands every 15 minutes between the hours of 1pm and 11pm)

# Alerts for the state of our certificates
  - name: Certificates
    rules:
      - alert: core ssl external (non-letsencrypt) 10 days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA
          Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's
          Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) <
          10
        labels:
          severity: p1
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.instance }}'
          summary: Cert less than 10 days from expiring

      - alert: core ssl external (non-letsencrypt) 30 days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA
          Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's
          Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) <
          30
        labels:
          severity: p2
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.instance }}'
          summary: Cert less than 30 days from expiring

      - alert: core ssl external (letsencrypt) 05 days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA
          Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's
          Encrypt|R3).*"} - time())/24/60/60) < 5
        labels:
          severity: p1
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.instance }}'
          summary: Let's Encrypt cert less than 05 days from expiring

      - alert: core ssl external (letsencrypt) 10 days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA
          Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's
          Encrypt|R3).*"} - time())/24/60/60) < 10
        labels:
          severity: p2
          cluster: prod
          resolve: false
        annotations:
          summary: Let's Encrypt cert less than 10 days from expiring

      - alert: core ssl kubernetes internal (letsencrypt) 05 days
        expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60
          < 5
        labels:
          severity: p1
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.name }}'
          summary: Let's Encrypt Kubernetes cert less than 05 days from expiring

      - alert: core ssl kubernetes internal (letsencrypt) 10 days
        expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60
          < 10
        labels:
          severity: p2
          cluster: prod
          resolve: false
        annotations:
          resource: '{{ $labels.name }}'
          summary: Let's Encrypt Kubernetes cert less than 10 days from expiring
